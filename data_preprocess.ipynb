{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc025ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0af8f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f1e0a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and group by cap size\n",
    "stocks_screener = pd.read_csv('nasdaq_screener_1637428146284.csv') #https://www.nasdaq.com/market-activity/stocks/screener\n",
    "stocks_screener = stocks_screener[stocks_screener['Market Cap'].notna()]\n",
    "\n",
    "M = 10**6\n",
    "B = 10**9\n",
    "\n",
    "bins =  np.array([0,50*M,300*M,2*B,10*B,200*B,np.inf])\n",
    "ind = np.digitize(stocks_screener['Market Cap'], bins)\n",
    "\n",
    "stocks_screener['Cap Group'] = ind\n",
    "\n",
    "gb = stocks_screener.groupby('Cap Group')\n",
    "list_of_groupbed_stocks = [gb.get_group(x) for x in gb.groups]\n",
    "\n",
    "cap_list = ['nano','micro','small','medium','large','mega']\n",
    "stock_cap_dict = dict()\n",
    "for i in range(len(cap_list)):\n",
    "    stock_cap_dict[cap_list[i]] = list_of_groupbed_stocks[i]['Symbol']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5fdf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nano 1216\n",
      "micro 880\n",
      "small 1077\n",
      "medium 503\n",
      "large 225\n",
      "mega 19\n"
     ]
    }
   ],
   "source": [
    "for cap in cap_list:\n",
    "    print(cap + ' ' + str(len(stock_cap_dict[cap])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27188c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_stock_get_data(stock_dict = stock_cap_dict,cap = 'small',num_stocks = 100,want_log = True,start_date=\"2018-01-01\",\n",
    "                        end_date=\"2021-12-31\",cut_date = '2021-01-01' ):\n",
    "    \n",
    "    #length standards\n",
    "    stock_data = yf.download('AAPL', start=start_date, end=end_date,progress=False).reset_index()\n",
    "    standard_length = len(stock_data)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    resample_count = 0\n",
    "    \n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    \n",
    "    while count<num_stocks:\n",
    "        \n",
    "        length = 0\n",
    "        \n",
    "        resample_count-=1\n",
    "        \n",
    "        while length!=standard_length:\n",
    "            resample_count+=1\n",
    "            stock_sample =  list((stock_dict[cap]).sample(n=1))[0]\n",
    "            stock_data = stock_data.replace(0, np.nan)\n",
    "            stock_data = yf.download(stock_sample, start=start_date, end=end_date,progress = False).reset_index().dropna()\n",
    "            \n",
    "            length = len(stock_data)\n",
    "        \n",
    "        \n",
    "        stock_data['daily_return'] = (stock_data.Close /stock_data.Close.shift(-1)).dropna()\n",
    "        \n",
    "        stock_data['Log_daily_return'] = np.log(stock_data.Close /stock_data.Close.shift(-1).dropna())\n",
    "        \n",
    "        train_full, test_full = stock_data[stock_data.Date<cut_date],stock_data[stock_data.Date>=cut_date]\n",
    "        \n",
    "        if want_log:\n",
    "        \n",
    "            train_data[stock_sample],test_data[stock_sample] = np.array(train_full[['Date','Log_daily_return']]),np.array(test_full[['Date','Log_daily_return']].dropna())\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            train_data[stock_sample],test_data[stock_sample] = np.array(train_full[['Date','daily_return']]),np.array(test_full[['Date','daily_return']].dropna())\n",
    "        \n",
    "        count+=1\n",
    "        \n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c6c47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting data for hmm and garch model (with log return)\n",
    "metrics_df_concat = pd.DataFrame()\n",
    "\n",
    "for cap_size in cap_list:\n",
    "    result_metric_dict = {}\n",
    "    train_data, test_data = pick_stock_get_data(stock_dict = stock_cap_dict,cap = cap_size,num_stocks = 100,want_log = True,start_date=\"2018-01-01\",\n",
    "                        end_date=\"2021-12-31\",cut_date = '2021-01-01' )\n",
    "    #write train\n",
    "    a_file = open('processed_data/'+cap_size+\"_train.pkl\", \"wb\")\n",
    "    pickle.dump(train_data, a_file)\n",
    "    a_file.close()\n",
    "    \n",
    "    #write test\n",
    "    a_file = open('processed_data/'+cap_size+\"_test.pkl\", \"wb\")\n",
    "    pickle.dump(test_data, a_file)\n",
    "    a_file.close()\n",
    "#     metrics_df = pd.DataFrame(result_metric_dict).T\n",
    "#     metrics_df['Cap_size'] = cap_size\n",
    "    \n",
    "#     metrics_df_concat = metrics_df_concat.append(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23dacb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Failed download:\n",
      "- BREZR: None\n",
      "\n",
      "1 Failed download:\n",
      "- AMAOW: None\n",
      "\n",
      "1 Failed download:\n",
      "- IMAQR: None\n",
      "\n",
      "1 Failed download:\n",
      "- CLAQR: None\n",
      "\n",
      "1 Failed download:\n",
      "- IMAQR: None\n",
      "\n",
      "1 Failed download:\n",
      "- MCAER: None\n",
      "\n",
      "1 Failed download:\n",
      "- ESSCR: None\n"
     ]
    }
   ],
   "source": [
    "# getting data for arima model (without log return)\n",
    "metrics_df_concat = pd.DataFrame()\n",
    "\n",
    "for cap_size in cap_list:\n",
    "    result_metric_dict = {}\n",
    "    train_data, test_data = pick_stock_get_data(\n",
    "            stock_dict = stock_cap_dict,cap = cap_size,num_stocks = 100,\n",
    "            want_log = False,start_date=\"2018-01-01\",\n",
    "            end_date=\"2021-12-31\",cut_date = '2021-01-01' )\n",
    "    #write train\n",
    "    a_file = open('processed_data_arima/'+cap_size+\"_train.pkl\", \"wb\")\n",
    "    pickle.dump(train_data, a_file)\n",
    "    a_file.close()\n",
    "    \n",
    "    #write test\n",
    "    a_file = open('processed_data_arima/'+cap_size+\"_test.pkl\", \"wb\")\n",
    "    pickle.dump(test_data, a_file)\n",
    "    a_file.close()\n",
    "#     metrics_df = pd.DataFrame(result_metric_dict).T\n",
    "#     metrics_df['Cap_size'] = cap_size\n",
    "    \n",
    "#     metrics_df_concat = metrics_df_concat.append(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e991f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 dict_keys(['WVVIP', 'CBIO', 'CYCCP', 'CLRB', 'NSYS', 'CUEN', 'SOHO', 'PIXY', 'STRR', 'RIBT', 'ACOR', 'ALJJ', 'CYRN', 'NERV', 'SVVC', 'BKEPP', 'RBCN', 'PRPO', 'BANFP', 'DYNT', 'THMO', 'ISIG', 'DTST', 'OCC', 'ONVO', 'VLYPO', 'PALT', 'DFFN', 'ABIO', 'OXLCM', 'LIXT', 'IDRA', 'REED', 'ZIVO', 'PBCTP', 'NYMTN', 'ACER', 'PULM', 'MIND', 'CPTAG', 'PSTV', 'APDN', 'CVV', 'CNFR', 'SUMR', 'WVFC', 'YTEN', 'LMRKO', 'FITBI', 'NYMTP', 'MEDS', 'SGBX', 'AATC', 'MTBCP', 'CWBR', 'SOHOB', 'NVIV', 'AUTO', 'QUMU', 'XELB', 'GROM', 'CUBA', 'CYAN', 'BSQR', 'TANNI', 'BPOPM', 'EKSO', 'XBIO', 'CPIX', 'NURO', 'KAVL', 'NXTD', 'MARPS', 'NSEC', 'EVOL', 'TTNP', 'CRTD', 'RAND', 'IFMK', 'CHSCO', 'BBLG']) \n",
      "\n",
      "87 dict_keys(['NH', 'SEVN', 'OPNT', 'KRMD', 'CIZN', 'MFIN', 'REFR', 'ESCA', 'MBCN', 'FRAF', 'SYRS', 'FNWB', 'HDSN', 'OXSQ', 'MRCC', 'CLSN', 'BYFC', 'AESE', 'HSON', 'VIRC', 'CMCT', 'BCML', 'GEOS', 'VERB', 'NNBR', 'CALA', 'CTHR', 'SINO', 'EIGR', 'OTIC', 'HNRG', 'BEEM', 'PFX', 'XSPA', 'ALRN', 'LTRPA', 'AUBN', 'IDN', 'WTRH', 'ORRF', 'BIVI', 'FSFG', 'ZVO', 'SMTI', 'NXTP', 'FRGI', 'PRPH', 'SALM', 'AVGR', 'UONE', 'MINM', 'WVVI', 'MLVF', 'KINS', 'AREC', 'BOXL', 'LNDC', 'STRS', 'FBIZ', 'MRBK', 'OFED', 'UBFO', 'LFMD', 'MRAM', 'PFMT', 'SSBI', 'OBT', 'GIFI', 'CFBK', 'STCN', 'CTSO', 'TAST', 'PRTK', 'WEYS', 'ZEUS', 'OESX', 'PWFL', 'CWBC', 'GLYC', 'ATHX', 'BWEN', 'SAMG', 'GTIM', 'NWPX', 'TXMD', 'ADVM', 'HWBK']) \n",
      "\n",
      "92 dict_keys(['NATR', 'BAND', 'ZGNX', 'CPSI', 'ITIC', 'FLIC', 'MCFT', 'AXGN', 'PGEN', 'HSKA', 'KRNY', 'ALDX', 'CASA', 'HMST', 'TACO', 'MRSN', 'COWN', 'ABTX', 'QNST', 'BLMN', 'CLVS', 'BDSI', 'KPTI', 'GLAD', 'WETF', 'MDGL', 'CHUY', 'ALT', 'WABC', 'SNEX', 'IEA', 'HCCI', 'PAVM', 'RBBN', 'HTBK', 'VRDN', 'RCKT', 'POWL', 'CNXN', 'QQQX', 'MYRG', 'DGII', 'CONN', 'NBTB', 'CSWC', 'SPTN', 'DMTK', 'OSIS', 'HIBB', 'CALM', 'TA', 'WTBA', 'FNLC', 'PLL', 'GEVO', 'MNKD', 'GERN', 'GTYH', 'CSII', 'PLAB', 'WRLD', 'INSG', 'CAC', 'BLBD', 'OSPN', 'UVSP', 'TTMI', 'CSTR', 'THFF', 'NSSC', 'MPAA', 'GABC', 'PTGX', 'CRMT', 'NEWT', 'PTSI', 'EHTH', 'TCMD', 'HBMD', 'RBB', 'EYPT', 'AMOT', 'PRDO', 'INVA', 'THRY', 'APEI', 'TPIC', 'HCKT', 'CZNC', 'TWOU', 'SCOR', 'VECO']) \n",
      "\n",
      "87 dict_keys(['ACHC', 'RCM', 'OZK', 'TXRH', 'CORT', 'UNIT', 'IIVI', 'IDCC', 'ABCB', 'PLXS', 'NBIX', 'ACIW', 'AEIS', 'ARNA', 'IPAR', 'ALTR', 'GT', 'APLS', 'FOLD', 'POWI', 'RGLD', 'IPGP', 'FCNCA', 'BL', 'CASY', 'SFIX', 'NTCT', 'LILA', 'SFNC', 'RILY', 'TNDM', 'DORM', 'PNFP', 'MMSI', 'SABR', 'WEN', 'SKYW', 'CRUS', 'TOWN', 'LILAK', 'NXST', 'FIBK', 'INSM', 'UCTT', 'DNLI', 'RETA', 'TFSL', 'SGMS', 'AMBA', 'THRM', 'MYGN', 'CSQ', 'XNCR', 'FELE', 'APPS', 'AMED', 'SIGI', 'AIMC', 'ONB', 'QDEL', 'BANF', 'FATE', 'VRNS', 'FCEL', 'QLYS', 'MGEE', 'EYE', 'WAFD', 'INDB', 'EXEL', 'CWST', 'HMHC', 'ATSG', 'MNDT', 'RPD', 'PDCO', 'HALO', 'CENTA', 'TTEC', 'BHF', 'RRR', 'RNST', 'AAON', 'PCH', 'CDXS', 'UTHR', 'EVBG']) \n",
      "\n",
      "79 dict_keys(['SNPS', 'MNST', 'ORLY', 'LAMR', 'VRSK', 'IDXX', 'DISCA', 'ILMN', 'GLPI', 'CSX', 'SSNC', 'LSXMK', 'POOL', 'VRTX', 'MDLZ', 'RGEN', 'LBRDA', 'INTU', 'VIAC', 'AEP', 'ATVI', 'XLNX', 'UAL', 'IEP', 'HAS', 'FWONK', 'DISCK', 'CG', 'LSXMA', 'MU', 'MPWR', 'CTXS', 'TSCO', 'DISCB', 'CZR', 'CDW', 'SPLK', 'MAR', 'SGEN', 'NTRS', 'AXON', 'FSLR', 'RUN', 'ZBRA', 'TER', 'ULTA', 'ETSY', 'ENTG', 'CSGP', 'MDB', 'SWKS', 'NWS', 'PANW', 'TMUS', 'EXC', 'TTWO', 'REGN', 'UHAL', 'TECH', 'Z', 'PTC', 'BMRN', 'PFG', 'ABMD', 'ERIE', 'HON', 'WDAY', 'XRAY', 'NLOK', 'KDP', 'ENPH', 'HSIC', 'EXAS', 'KLAC', 'HST', 'CTAS', 'MORN', 'MKTX', 'NTAP']) \n",
      "\n",
      "19 dict_keys(['AVGO', 'PYPL', 'PEP', 'GOOG', 'NVDA', 'TSLA', 'COST', 'QCOM', 'GOOGL', 'ADBE', 'AAPL', 'MSFT', 'FB', 'AMZN', 'CSCO', 'CMCSA', 'INTC', 'NFLX', 'ADI']) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cap in cap_list:\n",
    "    a_file = open(\"processed_data/{}_train.pkl\".format(cap), \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    print(len(output), output.keys(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4801acc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 dict_keys(['SNOA', 'CYCCP', 'AATC', 'JSM', 'JCTCF', 'NTRB', 'NVFY', 'LMRKO', 'WVFC', 'ARTW', 'MIND', 'BSQR', 'POAI', 'GROM', 'OPHC', 'EFOI', 'RAVE', 'RMTI', 'DHCNI', 'TANNI', 'USEG', 'VIVE', 'VIASP', 'TANNZ', 'SCKT', 'BANFP', 'GVP', 'CLRB', 'TAYD', 'WINT', 'BBI', 'BKYI', 'WTFCM', 'ZIONP', 'LIXT', 'LPTH', 'SLRX', 'SSNT', 'JAN', 'CLRO', 'EVOL', 'FNHC', 'NURO', 'OBLG', 'SINT', 'KTRA', 'STAF', 'EAST', 'CYCC', 'CYAN', 'SGLB', 'CPIX', 'QUMU', 'FITBI', 'XBIO', 'CREX', 'XELB', 'CHSCN', 'HOVNP', 'ABIO', 'STRR', 'ONVO', 'KPRX', 'CPTAL', 'NDRA', 'DTST', 'KEQU', 'THMO', 'EVOK', 'HVBC', 'BNTC', 'MSVB', 'NYMTP', 'AUTO', 'PULM', 'CNFR', 'HTGM', 'AHPI', 'HSDT']) \n",
      "\n",
      "88 dict_keys(['VABK', 'AVEO', 'ADES', 'WLFC', 'GFED', 'SPPI', 'ASMB', 'MRLN', 'MLVF', 'CKPT', 'LWAY', 'MRKR', 'EVFM', 'REFR', 'NBN', 'CSSE', 'ARAV', 'CHMG', 'RVSB', 'PMTS', 'CRVS', 'MRBK', 'ESPR', 'NAII', 'MAYS', 'LTRX', 'AGRX', 'DARE', 'SNCR', 'OPOF', 'KMPH', 'LMNR', 'SEVN', 'HROW', 'SGA', 'NBEV', 'VTVT', 'NCSM', 'RNDB', 'OMEX', 'MCHX', 'LIFE', 'EML', 'HTBX', 'AIKI', 'FVCB', 'WSTG', 'ACHV', 'SYBX', 'PTMN', 'USAU', 'FCRD', 'CLSN', 'MGYR', 'LMFA', 'BFIN', 'LTBR', 'FVE', 'ITI', 'BTCS', 'BEEM', 'VTNR', 'FNWB', 'OCX', 'LFMD', 'HDSN', 'UONEK', 'PNRG', 'SMTI', 'LPCN', 'ADXS', 'PMCB', 'QUIK', 'OPGN', 'DLPN', 'CECE', 'CGRN', 'FPAY', 'CEMI', 'ELOX', 'CTHR', 'BCBP', 'CPSS', 'KOSS', 'VERB', 'AGTC', 'LIVE', 'NKSH']) \n",
      "\n",
      "94 dict_keys(['EBTC', 'FRTA', 'VKTX', 'STRA', 'ZUMZ', 'PRAA', 'LWLG', 'ATRS', 'AMOT', 'CASS', 'DGICB', 'OSPN', 'GRBK', 'SWBI', 'ASTE', 'LCUT', 'SLP', 'SRCE', 'HBIO', 'SENEA', 'RLMD', 'KBAL', 'POWL', 'CERS', 'AMSWA', 'PFBC', 'PLSE', 'ERII', 'CSGS', 'ENTA', 'MOFG', 'APEN', 'SRDX', 'PRIM', 'CARA', 'BATRA', 'NBTB', 'VNOM', 'BSRR', 'CNXN', 'GRPN', 'NMFC', 'FOSL', 'CCRN', 'TTMI', 'TRST', 'AMCX', 'UPLD', 'TACO', 'VNDA', 'SMMF', 'KIRK', 'XPER', 'WKHS', 'TCPC', 'NTUS', 'CHI', 'OSUR', 'STKS', 'YELL', 'AGLE', 'ORGO', 'OSIS', 'AXTI', 'OPRX', 'CEVA', 'NRC', 'MLAB', 'COLL', 'NMIH', 'GOGO', 'MRSN', 'CZNC', 'TRHC', 'RDUS', 'ANAB', 'BJRI', 'VERI', 'RMBL', 'TITN', 'SLRC', 'AERI', 'BCOV', 'LXRX', 'CFFN', 'FBNC', 'CLFD', 'RDIB', 'UVSP', 'SMSI', 'CHY', 'IEA', 'NFBK', 'UFPT']) \n",
      "\n",
      "92 dict_keys(['NAVI', 'ATRC', 'SMPL', 'ATSG', 'BRKS', 'FELE', 'ALRM', 'CSQ', 'QRTEB', 'NEOG', 'OTTR', 'VICR', 'AVT', 'ANAT', 'MANT', 'MARA', 'APPF', 'JJSF', 'SLAB', 'MGLN', 'HOMB', 'TRMK', 'REGI', 'PPBI', 'IONS', 'CHDN', 'PZZA', 'PNFP', 'ITCI', 'MKSI', 'RMBS', 'BBBY', 'RCM', 'SIGI', 'EDIT', 'FCEL', 'NSTG', 'UCTT', 'HTLF', 'ODP', 'CORT', 'HAIN', 'SFM', 'SMTC', 'CWST', 'RPD', 'LGND', 'SSB', 'TCBI', 'MGEE', 'VLY', 'MGNI', 'SGMS', 'NATI', 'TTGT', 'PRFT', 'EYE', 'EXEL', 'DRNA', 'RILY', 'IOSP', 'CARG', 'LOB', 'FULT', 'ROCK', 'FIZZ', 'GBCI', 'UBSI', 'CENTA', 'MAT', 'COKE', 'CBSH', 'FCFS', 'VRNT', 'SATS', 'CSWI', 'ONB', 'BCPC', 'WSC', 'PACB', 'PCRX', 'WIRE', 'HQY', 'CACC', 'FIBK', 'BLFS', 'APPN', 'PCH', 'SLM', 'CELH', 'CCOI', 'MNRO']) \n",
      "\n",
      "76 dict_keys(['FFIV', 'GILD', 'FIVE', 'AMAT', 'TXN', 'COUP', 'SBNY', 'BIIB', 'BRKR', 'TTD', 'DXCM', 'EXPE', 'LKQ', 'CERN', 'SIRI', 'ALGN', 'LBRDA', 'AMGN', 'MDB', 'NDSN', 'CHTR', 'CROX', 'CDNS', 'ZION', 'LAMR', 'OKTA', 'LNT', 'VTRS', 'NLOK', 'EBAY', 'STLD', 'JBHT', 'LSXMB', 'FWONA', 'ENPH', 'AEP', 'SIVB', 'CG', 'PODD', 'HOLX', 'EXPD', 'CPRT', 'NUAN', 'ATVI', 'SYNA', 'NWSA', 'RGEN', 'DLTR', 'EXC', 'VRSN', 'CDW', 'CAR', 'TMUS', 'INTU', 'KLAC', 'NTAP', 'SNPS', 'SPLK', 'ODFL', 'INCY', 'FTNT', 'WYNN', 'ABMD', 'TECH', 'NDAQ', 'GLPI', 'XRAY', 'NWS', 'EXAS', 'XEL', 'AAL', 'MORN', 'CSX', 'CINF', 'SYNH', 'LSXMK']) \n",
      "\n",
      "19 dict_keys(['GOOG', 'NFLX', 'ADI', 'CSCO', 'FB', 'CMCSA', 'AVGO', 'AMZN', 'PYPL', 'INTC', 'GOOGL', 'ADBE', 'AAPL', 'PEP', 'MSFT', 'TSLA', 'COST', 'QCOM', 'NVDA']) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for cap in cap_list:\n",
    "    a_file = open(\"processed_data_arima/{}_train.pkl\".format(cap), \"rb\")\n",
    "    output = pickle.load(a_file)\n",
    "    print(len(output), output.keys(), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
